#!/usr/bin/env python3

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib
import os
import time
from flask import Flask, request, jsonify, send_file
from flask_swagger_ui import get_swaggerui_blueprint
import threading
import socket
import psutil

app = Flask(__name__)

os.makedirs('static', exist_ok=True)
with open('static/swagger.json', 'w') as f:
    f.write('''{
      "swagger": "2.0",
      "info": {
        "title": "ML Model Service API",
        "version": "1.0.0"
      },
      "paths": {
        "/health": {
          "get": {
            "summary": "Health check endpoint"
          }
        },
        "/download-model": {
          "get": {
            "summary": "Download the trained model"
          }
        },
        "/model-info": {
          "get": {
            "summary": "Get information about the model"
          }
        }
      }
    }''')

SWAGGER_URL = '/swagger'
API_URL = '/static/swagger.json'
swaggerui_blueprint = get_swaggerui_blueprint(
    SWAGGER_URL,
    API_URL,
    config={
        'app_name': "ML Model Service"
    }
)
app.register_blueprint(swaggerui_blueprint, url_prefix=SWAGGER_URL)

class ModelTrainer:
    def __init__(self):
        self.model = RandomForestClassifier(n_estimators=100, random_state=42)
        self.model_lock = threading.Lock()
        self.load_model()

    def load_model(self, model_dir="models"):
        # FIXED: Create models directory if it doesn't exist
        os.makedirs(model_dir, exist_ok=True)
        model_path = os.path.join(model_dir, "random_forest_model.joblib")
        if os.path.exists(model_path):
            with self.model_lock:
                self.model = joblib.load(model_path)
                print(f"‚úîÔ∏è Model loaded from {model_path}")
        else:
            print(f"‚ö†Ô∏è No existing model found at {model_path}, will train a new one")

    def train(self, X_train, X_test, y_train, y_test):
        print("\nüîπ Training Random Forest model...")
        with self.model_lock:
            self.model.fit(X_train, y_train)
            y_pred = self.model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"üîπ Accuracy: {accuracy:.4f}")
            print("üîπ Classification Report:")
            print(classification_report(y_test, y_pred))
            self.save_model()

    def save_model(self, output_dir="models"):
        try:
            print(f"üìù Attempting to save model to directory: {output_dir}")
            abs_output_dir = os.path.abspath(output_dir)
            print(f"üìù Absolute path: {abs_output_dir}")
            
            # Monitor memory usage
            process = psutil.Process(os.getpid())
            mem_before = process.memory_info().rss / 1024 / 1024  # Convert to MB
            print(f"üìä Memory usage before save: {mem_before:.2f} MB")
            
            # Create directory with verbose output
            try:
                os.makedirs(abs_output_dir, exist_ok=True)
                print(f"üìÅ Created/verified directory: {abs_output_dir}")
                print(f"üìÅ Directory permissions: {oct(os.stat(abs_output_dir).st_mode)[-3:]}")
            except Exception as dir_error:
                print(f"‚ùå Error creating directory: {str(dir_error)}")
                raise
            
            model_path = os.path.join(abs_output_dir, "random_forest_model.joblib")
            print(f"üìù Full model path: {model_path}")
            
            print("üíæ Starting model dump...")
            try:
                # Use compression to reduce memory usage during save
                temp_path = model_path + '.tmp'
                print(f"üíæ Saving to temporary file: {temp_path}")
                
                # Try to save with explicit file opening
                with open(temp_path, 'wb') as f:
                    joblib.dump(self.model, f, compress=3)
                
                mem_after = process.memory_info().rss / 1024 / 1024
                print(f"üìä Memory usage after dump: {mem_after:.2f} MB (Change: {mem_after - mem_before:.2f} MB)")
                
                print("‚úÖ Temporary file saved successfully")
                print(f"üìÅ Temporary file permissions: {oct(os.stat(temp_path).st_mode)[-3:]}")
                
                os.replace(temp_path, model_path)
                print("‚úÖ Model file moved to final location")
                
                if os.path.exists(model_path):
                    size = os.path.getsize(model_path)
                    print(f"‚úîÔ∏è Model saved successfully to {model_path} (size: {size:,} bytes)")
                    print(f"üìÅ Final file permissions: {oct(os.stat(model_path).st_mode)[-3:]}")
                else:
                    raise FileNotFoundError(f"Model file not found after moving from temporary location")
            except Exception as save_error:
                print(f"‚ùå Error during model save operation: {str(save_error)}")
                if os.path.exists(temp_path):
                    try:
                        os.remove(temp_path)
                        print("üßπ Cleaned up temporary file")
                    except Exception as cleanup_error:
                        print(f"‚ö†Ô∏è Failed to clean up temporary file: {str(cleanup_error)}")
                raise
        except Exception as e:
            print(f"‚ùå Error in save_model: {str(e)}")
            print(f"üìÅ Current working directory: {os.getcwd()}")
            print(f"üìÅ Directory listing of {output_dir}:")
            try:
                print(os.listdir(output_dir))
            except Exception as dir_error:
                print(f"(unable to list directory: {str(dir_error)})")
            print("üìÅ Directory listing of current directory:")
            print(os.listdir('.'))

def process_dataset(dataset_path, trainer):
    time.sleep(1)
    try:
        print(f"üìå Processing dataset: {dataset_path}")
        df = pd.read_csv(dataset_path, low_memory=False)
        
        # Check if 'Abnormality class' column exists
        if 'Abnormality class' not in df.columns:
            print(f"‚ùå Error: Required column 'Abnormality class' not found in dataset")
            print(f"Available columns: {df.columns.tolist()}")
            return
            
        df = df.dropna(subset=['Abnormality class'])
        print("üìå Data shape:", df.shape)
        print("üìå Class distribution:")
        print(df['Abnormality class'].value_counts())
        
        # FIXED: Safer column dropping approach
        # Only drop columns that actually exist in the dataframe
        drop_cols = ['timestamp', 'Microservice', 'Experiment']
        if 'Unnamed: 0' in df.columns:
            drop_cols.append('Unnamed: 0')
            
        # Find deployed columns
        deployed_cols = [col for col in df.columns if col.endswith('_deployed_at')]
        drop_cols.extend(deployed_cols)
        
        # Make sure we don't try to drop columns that don't exist
        existing_drop_cols = [col for col in drop_cols if col in df.columns]
        
        # Make sure 'Abnormality class' exists before trying to drop it
        columns_to_drop = existing_drop_cols.copy()
        if 'Abnormality class' in df.columns:
            columns_to_drop.append('Abnormality class')
            
        X = df.drop(columns_to_drop, axis=1)
        y = df['Abnormality class']
        X = X.apply(pd.to_numeric, errors='coerce')
        X = X.fillna(0)
        print("üìå Features shape after preprocessing:", X.shape)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        print(f"‚úîÔ∏è Training set size: {X_train.shape}")
        print(f"‚úîÔ∏è Test set size: {X_test.shape}")
        trainer.train(X_train, X_test, y_train, y_test)
    except Exception as e:
        print(f"‚ùå Error processing dataset: {str(e)}")

@app.route('/health')
def health():
    return jsonify({"status": "healthy"}), 200

@app.route('/download-model', methods=['GET'])
def download_model():
    model_path = os.path.join("models", "random_forest_model.joblib")
    if os.path.exists(model_path):
        modified_time = time.ctime(os.path.getmtime(model_path))
        try:
            response = send_file(
                model_path,
                mimetype="application/octet-stream",
                as_attachment=True,
                download_name="random_forest_model.joblib"
            )
            response.headers["X-Model-Last-Modified"] = modified_time
            return response
        except Exception as e:
            return jsonify({"error": f"Error sending file: {str(e)}"}), 500
    else:
        return jsonify({"error": "Model file not found"}), 404

@app.route('/model-info', methods=['GET'])
def model_info():
    model_path = os.path.join("models", "random_forest_model.joblib")
    if os.path.exists(model_path):
        file_stats = os.stat(model_path)
        return jsonify({
            "model_path": model_path,
            "last_modified": time.ctime(file_stats.st_mtime),
            "size_bytes": file_stats.st_size
        })
    else:
        return jsonify({"error": "Model file not found"}), 404

# FIXED: Add port availability check
def is_port_available(port, host='0.0.0.0'):
    """Check if a port is available on the given host."""
    try:
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.bind((host, port))
        sock.close()
        return True
    except OSError:
        return False

def find_available_port(start_port=8080, max_attempts=10):
    """Find an available port starting from start_port."""
    for port in range(start_port, start_port + max_attempts):
        if is_port_available(port):
            return port
    raise OSError(f"No available ports found between {start_port} and {start_port + max_attempts - 1}")

def main():
    try:
        # Verify required libraries are installed
        required_libraries = ['pandas', 'numpy', 'sklearn', 'joblib', 'flask', 'flask_swagger_ui']
        missing_libraries = []
        
        for lib in required_libraries:
            try:
                __import__(lib)
            except ImportError:
                missing_libraries.append(lib)
        
        if missing_libraries:
            print(f"‚ùå Error: Missing required libraries: {', '.join(missing_libraries)}")
            print("Please install them using: pip install " + " ".join(missing_libraries))
            return
            
        trainer = ModelTrainer()
        if os.path.exists('final_dataset.csv'):
            print("üìã Found existing dataset, processing...")
            process_dataset('final_dataset.csv', trainer)
        else:
            print("‚ö†Ô∏è Dataset 'final_dataset.csv' not found. The API will be started without training.")
        
        port = find_available_port(8080)
        print(f"üöÄ Starting API server on port {port} (with Swagger)...")
        app.run(host='0.0.0.0', port=port, debug=False)
    except OSError as e:
        print(f"‚ùå Failed to start server: {e}")
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")

if __name__ == "__main__":
    main()