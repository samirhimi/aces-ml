import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.impute import SimpleImputer
import argparse
import joblib  # For saving the model
from joblib import dump, load

# ğŸ“‚ Charger le dataset
df = pd.read_csv("final_dataset.csv")

# ğŸ” VÃ©rifier la distribution des classes
print("ğŸ“Œ Distribution des classes :\n", df["Abnormality class"].value_counts())

# ğŸ¯ DÃ©finir X (features) et y (target)
print("ğŸ“Œ Colonnes disponibles :", df.columns)
X = df.drop(columns=["Abnormality class", "Experiment"], errors="ignore")
y = df["Abnormality class"]

# ğŸ­ Encodage des labels (y)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

# ğŸ”„ Convert non-numeric columns to numeric
non_numeric_columns = X.select_dtypes(include=['object']).columns
for col in non_numeric_columns:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col])

# ğŸš¨ Handle missing values
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)

# ğŸ² SÃ©parer en train & test (80% train, 20% test) avec stratification
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# ğŸ”¬ Normalisation des features (important pour SVM et KNN)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# âœ… VÃ©rification
print(f"âœ”ï¸ Taille du jeu de train : {X_train.shape}")
print(f"âœ”ï¸ Taille du jeu de test : {X_test.shape}")

# ğŸ“Œ Initialisation des modÃ¨les
models = {
    "Random Forest": RandomForestClassifier(),
    "Decision Tree": DecisionTreeClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier(),
    "Naive Bayes": GaussianNB()
}

# ğŸ”¥ EntraÃ®nement et Ã©valuation des modÃ¨les
accuracies = {}
for name, model in models.items():
    print(f"\nğŸ”¹ EntraÃ®nement du modÃ¨le {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    # ğŸ¯ Stocker l'accuracy
    accuracies[name] = accuracy_score(y_test, y_pred)
    
    # ğŸ“Š Affichage des rÃ©sultats
    print(f"ğŸ“Œ ModÃ¨le : {name}")
    print(f"ğŸ”¹ Accuracy : {accuracies[name]:.4f}")
    print(f"ğŸ”¹ Rapport de classification : \n{classification_report(y_test, y_pred, target_names=label_encoder.classes_)}")
    print("-" * 50)

# ğŸ“Š Tracer les rÃ©sultats
plt.figure(figsize=(8, 5))
plt.bar(accuracies.keys(), accuracies.values(), color=['blue', 'orange', 'green', 'red', 'purple'])
plt.xlabel("ModÃ¨les")
plt.ylabel("Accuracy")
plt.title("Performance des ModÃ¨les de Machine Learning")
plt.ylim(0, 1)
plt.xticks(rotation=30)
plt.show()


model = KNeighborsClassifier()
#model = LogisticRegression(solver='liblinear', multi_class='ovr')
model.fit(X_train, y_train)


# 10. Hacer predicciones con el conjunto de validaciÃ³n
# Make predictions on the validation set
predictions = model.predict(X_test)
predictions = predictions.round().astype(int)

# Save the model
dump(model, 'model_KNN.joblib') 

'''
# Load the model later
loaded_model = load('model_KNN.joblib')


sample_features = X_test[10]
sample_features = np.array(sample_features).reshape(1, -1)

# 3. Make prediction
prediction = loaded_model.predict(sample_features)

# If your model does probabilities:
# probabilities = model.predict_proba(sample_features)

print(f"Predicted class: {prediction[0]}")'''